#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–‡ç« ç”Ÿæˆå™¨ - æ ¸å¿ƒå·¥ä½œæµå¤„ç†å™¨
Article Generator - Core Workflow Processor
"""

from celery import Task
from typing import Dict, List, Any
import asyncio
import aiohttp
import json
from datetime import datetime
from loguru import logger
from ..core.task_scheduler import celery_app
from ..core.config import settings
from ..core.database import db_manager, cache_manager
from ..services.stock_data_service import StockDataService
from ..services.ai_agent_pool import AIAgentPool

class ArticleGenerationTask(Task):
    """æ–‡ç« ç”Ÿæˆä»»åŠ¡åŸºç±»"""
    
    def on_failure(self, exc, task_id, args, kwargs, einfo):
        """ä»»åŠ¡å¤±è´¥æ—¶çš„å›è°ƒ"""
        logger.error(f"âŒ ä»»åŠ¡å¤±è´¥ {task_id}: {exc}")
        
    def on_success(self, retval, task_id, args, kwargs):
        """ä»»åŠ¡æˆåŠŸæ—¶çš„å›è°ƒ"""
        logger.info(f"âœ… ä»»åŠ¡æˆåŠŸå®Œæˆ {task_id}")

@celery_app.task(bind=True, base=ArticleGenerationTask, name="prism.workers.article_generator.generate_articles_workflow")
def generate_articles_workflow(self, stock_code: str, options: Dict[str, Any]):
    """
    æ–‡ç« ç”Ÿæˆå·¥ä½œæµ - è¿™æ˜¯æ›¿ä»£n8nçš„æ ¸å¿ƒä¸šåŠ¡é€»è¾‘
    """
    task_id = self.request.id
    
    try:
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºè¿›è¡Œä¸­
        asyncio.run(db_manager.update_task_status(task_id, "running"))
        
        # è®¾ç½®è¿›åº¦å›è°ƒ
        self.update_state(state="PROGRESS", meta={"step": "åˆå§‹åŒ–", "progress": 10})
        
        # 1. åˆå§‹åŒ–æœåŠ¡
        stock_service = StockDataService()
        ai_agent_pool = AIAgentPool()
        
        # 2. å¹¶è¡Œè·å–è‚¡ç¥¨æ•°æ®
        self.update_state(state="PROGRESS", meta={"step": "è·å–è‚¡ç¥¨æ•°æ®", "progress": 20})
        
        stock_data = asyncio.run(collect_stock_data(stock_service, stock_code))
        
        if not stock_data:
            raise Exception(f"æ— æ³•è·å–è‚¡ç¥¨ {stock_code} çš„æ•°æ®")
        
        # 3. å¹¶è¡Œè°ƒç”¨AI Agentç”Ÿæˆåˆ†æ
        self.update_state(state="PROGRESS", meta={"step": "AIåˆ†æå¤„ç†", "progress": 40})
        
        ai_analyses = asyncio.run(process_ai_analysis(ai_agent_pool, stock_data, options))
        
        # 4. ç”Ÿæˆå¤šç¯‡æ–‡ç« 
        self.update_state(state="PROGRESS", meta={"step": "ç”Ÿæˆæ–‡ç« ", "progress": 70})
        
        articles = asyncio.run(generate_multiple_articles(ai_analyses, stock_data, options))
        
        # 5. ç»“æœèšåˆå’Œæ ¼å¼åŒ–
        self.update_state(state="PROGRESS", meta={"step": "ç»“æœå¤„ç†", "progress": 90})
        
        result = {
            "task_id": task_id,
            "stock_code": stock_code,
            "articles": articles,
            "metadata": {
                "total_articles": len(articles),
                "processing_time": datetime.now().isoformat(),
                "data_sources": list(stock_data.keys()) if stock_data else [],
                "ai_models_used": len(ai_analyses)
            },
            "status": "completed"
        }
        
        # ä¿å­˜ç»“æœåˆ°æ•°æ®åº“
        asyncio.run(db_manager.update_task_status(task_id, "completed", result))
        
        self.update_state(state="SUCCESS", meta={"step": "å®Œæˆ", "progress": 100})
        
        logger.info(f"ğŸ‰ æ–‡ç« ç”Ÿæˆå·¥ä½œæµå®Œæˆ: {task_id}")
        return result
        
    except Exception as e:
        # é”™è¯¯å¤„ç†
        error_msg = f"æ–‡ç« ç”Ÿæˆå¤±è´¥: {str(e)}"
        logger.error(f"âŒ {error_msg}")
        
        asyncio.run(db_manager.update_task_status(task_id, "failed", {"error": error_msg}))
        
        raise self.retry(exc=e, countdown=60, max_retries=3)

async def collect_stock_data(stock_service: StockDataService, stock_code: str) -> Dict[str, Any]:
    """
    å¹¶è¡Œæ”¶é›†è‚¡ç¥¨æ•°æ® - æ›¿ä»£n8nä¸­çš„å¤šä¸ªHTTPèŠ‚ç‚¹
    """
    tasks = [
        stock_service.get_fundamental_data(stock_code),
        stock_service.get_technical_data(stock_code),
        stock_service.get_financial_data(stock_code),
        stock_service.get_market_sentiment(stock_code)
    ]
    
    try:
        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰æ•°æ®è·å–ä»»åŠ¡
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        stock_data = {}
        data_types = ["fundamental", "technical", "financial", "sentiment"]
        
        for i, result in enumerate(results):
            if not isinstance(result, Exception):
                stock_data[data_types[i]] = result
            else:
                logger.warning(f"âš ï¸ è·å–{data_types[i]}æ•°æ®å¤±è´¥: {result}")
        
        logger.info(f"ğŸ“Š æ”¶é›†åˆ° {len(stock_data)} ç§ç±»å‹çš„è‚¡ç¥¨æ•°æ®")
        return stock_data
        
    except Exception as e:
        logger.error(f"âŒ è‚¡ç¥¨æ•°æ®æ”¶é›†å¤±è´¥: {e}")
        return {}

async def process_ai_analysis(ai_pool: AIAgentPool, stock_data: Dict[str, Any], options: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    å¹¶è¡Œå¤„ç†AIåˆ†æ - æ›¿ä»£n8nä¸­çš„AIèŠ‚ç‚¹è°ƒç”¨
    """
    analysis_tasks = []
    
    # ä¸ºæ¯ç§æ–‡ç« é£æ ¼åˆ›å»ºAIåˆ†æä»»åŠ¡
    for style in options.get("article_styles", settings.ARTICLE_STYLES):
        task = ai_pool.analyze_stock_data(stock_data, style)
        analysis_tasks.append(task)
    
    # å¹¶è¡Œæ‰§è¡ŒAIåˆ†æ
    results = await asyncio.gather(*analysis_tasks, return_exceptions=True)
    
    successful_analyses = []
    for i, result in enumerate(results):
        if not isinstance(result, Exception):
            successful_analyses.append(result)
        else:
            logger.warning(f"âš ï¸ AIåˆ†æå¤±è´¥ (é£æ ¼{i}): {result}")
    
    logger.info(f"ğŸ¤– å®Œæˆ {len(successful_analyses)} ä¸ªAIåˆ†æ")
    return successful_analyses

async def generate_multiple_articles(ai_analyses: List[Dict[str, Any]], stock_data: Dict[str, Any], options: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    ç”Ÿæˆå¤šç¯‡æ–‡ç«  - æ›¿ä»£n8nä¸­çš„æ–‡ç« ç”Ÿæˆå’Œæ ¼å¼åŒ–èŠ‚ç‚¹
    """
    articles = []
    
    for i, analysis in enumerate(ai_analyses):
        try:
            article = {
                "id": f"article_{i+1}",
                "style": analysis.get("style", "professional"),
                "title": analysis.get("title", f"è‚¡ç¥¨åˆ†ææŠ¥å‘Š #{i+1}"),
                "content": analysis.get("content", ""),
                "summary": analysis.get("summary", ""),
                "recommendations": analysis.get("recommendations", []),
                "confidence_score": analysis.get("confidence_score", 0.0),
                "data_sources": list(stock_data.keys()),
                "generated_at": datetime.now().isoformat(),
                "word_count": len(analysis.get("content", ""))
            }
            
            articles.append(article)
            
        except Exception as e:
            logger.error(f"âŒ æ–‡ç« ç”Ÿæˆå¤±è´¥ (ç¬¬{i+1}ç¯‡): {e}")
            continue
    
    logger.info(f"ğŸ“ ç”Ÿæˆäº† {len(articles)} ç¯‡æ–‡ç« ")
    return articles

# ä»»åŠ¡æ¸…ç†å’Œç»´æŠ¤
@celery_app.task(name="prism.workers.article_generator.cleanup_old_tasks")
def cleanup_old_tasks():
    """æ¸…ç†æ—§ä»»åŠ¡è®°å½•"""
    try:
        # è¿™é‡Œæ·»åŠ æ¸…ç†é€»è¾‘
        logger.info("ğŸ§¹ æ‰§è¡Œä»»åŠ¡æ¸…ç†")
        return {"status": "completed", "cleaned_tasks": 0}
    except Exception as e:
        logger.error(f"âŒ ä»»åŠ¡æ¸…ç†å¤±è´¥: {e}")
        return {"status": "failed", "error": str(e)}